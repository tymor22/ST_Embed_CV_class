import torch 
import torchvision
import torch.nn as nn 
from IPython.display import Image 
from torchvision import transforms
import matplotlib.pyplot as plt
import random
from torch.utils.data import DataLoader
from tqdm.auto import tqdm
import torch.nn.functional as F

import sys
import os
import csv
import numpy as np

import torch
from torch.utils.data import Dataset
from torchvision.transforms import Compose, ToTensor

from PIL import Image

import pandas as pd
import linecache
import re

from count_vae import CountVAE 
from datasets import STDataset_triple

from cae_dcae import CAE

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
seed = 12345
random.seed(seed)
torch.manual_seed(seed)

########## Data set loading #########################################################

metafile = "../data/meta_train.tsv"
dat1 = STDataset_triple(metafile, iso_cutoff=1.2)
train_loader = DataLoader(dat1, batch_size=32, shuffle=False)

#parameters for cae
denoising = False
latent_dim = 100


########## Pretrained Vision Model loading, count VAE initialization ################## 

learning_rate = 1e-3
num_classes = latent_dim  #Dimension of the Histology representation we want
n_latent = 20
n_latent_concat = num_classes + n_latent


model = CAE(latent_dim=latent_dim).to(device)

#Initialize count VAE model
nnet_model = CountVAE(
            11162, n_layers = 1, n_latent=20, n_pretrained_features=num_classes,
            n_latent_concat=n_latent_concat, n_hidden=256, gene_likelihood="zinb", dropout_rate=.1,
            n_labels=1
        ).to(device)


######### Parameters to optimize for ##################################################

adam_param_list = []
adam_param_list.append({'params': model.parameters()})
adam_param_list.append({'params': nnet_model.parameters()})


learning_rate = 1e-3
optimiser = torch.optim.Adam(adam_param_list, lr=learning_rate)


######## Training of model ############################################################

triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)

def add_noise(images):
    images_noisy = torch.clamp((images + torch.randn(images.shape) * torch.rand(images.shape)), -1, 1)
    return images_noisy

def train_joint_vae(nnet, model, train_loader, optimiser, epoch, num_epoch, learning_rate, log_interval):

    count_loss_sum = 0
    trip_loss_sum = 0
    net_loss_sum = 0
    triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)
    criteria = nn.MSELoss(reduction='sum')

    for i, sample_batched in enumerate(train_loader):
        
        optimiser.zero_grad()
        
        images, data, annotation, k_distant, k_near = sample_batched
        images = images.to(device)
        data = data.type(torch.float32).to(device) 
        library = torch.log(torch.sum(data, 1) + 1.0).unsqueeze(1).to(device)
        k_dist_img = k_distant[0][:,0,:,:].to(device)
        k_dist_count = k_distant[1][:,0,:].type(torch.float32).to(device)
        k_near_img = k_near[0][:,0,:,:].to(device)
        k_near_count = k_near[1][:,0,:].type(torch.float32).to(device)
        
        if denoising:
            input_images = add_noise(images).to(device)
        else:
            input_images = images.to(device)

        img_outputs = model(input_images)
        img_loss = criteria(img_outputs, images)
        
        #Sample Z- latent space from cae
        concat_z = model.sample_z(images).to(device)
        k_dist_concat_z = model.sample_z(k_dist_img).to(device)
        k_near_concat_z = model.sample_z(k_near_img).to(device)
        
        #Count vae and count loss
        reconst_loss_train, kl_divergence_train = nnet_model(data, library, concat_z)
        train_loss = torch.mean(reconst_loss_train + kl_divergence_train)
        
        #Sample cuont vae
        data_z = nnet_model.sample_from_posterior_z(data)[2].to(device)
        k_dist_z = nnet_model.sample_from_posterior_z(data)[2].to(device)
        k_near_z = nnet_model.sample_from_posterior_z(data)[2].to(device)
        
        trip_loss_count = triplet_loss(data_z_, k_near_z_comb, k_dist_z_comb)
        trip_loss_img = triplet_loss(concat_z, k_near_concat_z, k_dist_concat_z)

        combined_loss = train_loss + img_loss + trip_loss_img + trip_loss_count 

        combined_loss.backward()
        optimiser.step()
        
        
        if (i + 1) % log_interval == 0:
            print('Epoch [{}/{}], Train loss:{:.6f}'.format(
                epoch + 1, num_epoch, 
                combined_loss.item()
            ))

    save_path_nnet = "/mnt/home/thamamsy/projects/ST_Embed/models/joint_cae_sep/jcae_triplet_sep_nnet_" + str(epoch) + '.pth' 
    save_path_model = "/mnt/home/thamamsy/projects/ST_Embed/models/joint_cae_sep/jcae_triplet_sep_model_" + str(epoch) + '.pth'
    
    torch.save(
      nnet_model.state_dict(),
      save_path_nnet
    )

    torch.save(
      model.state_dict(),
      save_path_model
    )
    
#Train loop
num_epoch = 40
log_interval = 1


for epoch in range(num_epoch):
    train_joint_vae(nnet_model, model, train_loader, optimiser, epoch, num_epoch, learning_rate, log_interval)
    
    

